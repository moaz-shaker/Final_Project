{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04 - Supervised Learning - Classification Models\n",
        "\n",
        "This notebook covers:\n",
        "1. Loading feature-selected data\n",
        "2. Training multiple classification models:\n",
        "   - Logistic Regression\n",
        "   - Decision Tree\n",
        "   - Random Forest\n",
        "   - Support Vector Machine (SVM)\n",
        "3. Model evaluation and comparison\n",
        "4. Performance metrics and visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, roc_auc_score, confusion_matrix, \n",
        "                           classification_report, roc_curve)\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load feature-selected data\n",
        "print(\"Loading feature-selected data...\")\n",
        "X_train = joblib.load('../data/X_train_selected.pkl')\n",
        "X_test = joblib.load('../data/X_test_selected.pkl')\n",
        "y_train = joblib.load('../data/y_train.pkl')\n",
        "y_test = joblib.load('../data/y_test.pkl')\n",
        "selected_features = joblib.load('../models/selected_features.pkl')\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of training data:\")\n",
        "print(X_train.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "print(\"Training and evaluating models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    if auc is not None:\n",
        "        print(f\"AUC: {auc:.4f}\")\n",
        "    \n",
        "    print(f\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for name, result in results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'],\n",
        "        'Recall': result['recall'],\n",
        "        'F1-Score': result['f1'],\n",
        "        'AUC': result['auc'] if result['auc'] is not None else 'N/A'\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"Model Performance Comparison:\")\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Find best model based on F1-score\n",
        "best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']\n",
        "print(f\"\\nBest model based on F1-Score: {best_model_name}\")\n",
        "print(f\"Best F1-Score: {comparison_df['F1-Score'].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# 1. Performance metrics comparison\n",
        "plt.subplot(2, 3, 1)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.2\n",
        "\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    values = [result['accuracy'], result['precision'], result['recall'], result['f1']]\n",
        "    plt.bar(x + i*width, values, width, label=name, alpha=0.8)\n",
        "\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.xticks(x + width*1.5, metrics)\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# 2. Confusion matrices\n",
        "for i, (name, result) in enumerate(results.items()):\n",
        "    plt.subplot(2, 3, i+2)\n",
        "    cm = confusion_matrix(y_test, result['y_pred'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=['No Disease', 'Disease'],\n",
        "                yticklabels=['No Disease', 'Disease'])\n",
        "    plt.title(f'{name} - Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, result in results.items():\n",
        "    if result['y_pred_proba'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n",
        "        auc = result['auc']\n",
        "        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})', linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Feature importance for tree-based models\n",
        "tree_models = ['Decision Tree', 'Random Forest']\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, name in enumerate(tree_models):\n",
        "    if name in results:\n",
        "        plt.subplot(1, 2, i+1)\n",
        "        model = results[name]['model']\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature_names = X_train.columns\n",
        "            \n",
        "            # Sort by importance\n",
        "            indices = np.argsort(importance)[::-1]\n",
        "            \n",
        "            plt.barh(range(len(importance)), importance[indices], \n",
        "                    color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "            plt.yticks(range(len(importance)), [feature_names[j] for j in indices])\n",
        "            plt.xlabel('Feature Importance')\n",
        "            plt.title(f'{name} - Feature Importance')\n",
        "            plt.gca().invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save trained models and results\n",
        "import os\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "os.makedirs('../results', exist_ok=True)\n",
        "\n",
        "# Save individual models\n",
        "for name, result in results.items():\n",
        "    model_filename = f'../models/{name.lower().replace(\" \", \"_\")}_model.pkl'\n",
        "    joblib.dump(result['model'], model_filename)\n",
        "    print(f\"Saved {name} model to {model_filename}\")\n",
        "\n",
        "# Save all results\n",
        "joblib.dump(results, '../models/supervised_learning_results.pkl')\n",
        "joblib.dump(comparison_df, '../results/model_comparison.pkl')\n",
        "\n",
        "# Save evaluation metrics to text file\n",
        "with open('../results/evaluation_metrics.txt', 'w') as f:\n",
        "    f.write(\"Heart Disease Prediction - Model Evaluation Results\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    \n",
        "    for name, result in results.items():\n",
        "        f.write(f\"{name}:\\n\")\n",
        "        f.write(f\"  Accuracy: {result['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"  Precision: {result['precision']:.4f}\\n\")\n",
        "        f.write(f\"  Recall: {result['recall']:.4f}\\n\")\n",
        "        f.write(f\"  F1-Score: {result['f1']:.4f}\\n\")\n",
        "        if result['auc'] is not None:\n",
        "            f.write(f\"  AUC: {result['auc']:.4f}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "    \n",
        "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
        "    f.write(f\"Best F1-Score: {comparison_df['F1-Score'].max():.4f}\\n\")\n",
        "\n",
        "print(\"\\nSupervised learning completed and models saved!\")\n",
        "print(\"Files saved:\")\n",
        "print(\"- Individual model files in ../models/\")\n",
        "print(\"- ../models/supervised_learning_results.pkl\")\n",
        "print(\"- ../results/model_comparison.pkl\")\n",
        "print(\"- ../results/evaluation_metrics.txt\")\n",
        "\n",
        "# Display final summary\n",
        "print(f\"\\nFinal Summary:\")\n",
        "print(f\"- Best performing model: {best_model_name}\")\n",
        "print(f\"- Best F1-Score: {comparison_df['F1-Score'].max():.4f}\")\n",
        "print(f\"- Best Accuracy: {comparison_df['Accuracy'].max():.4f}\")\n",
        "print(f\"- All models trained and saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
